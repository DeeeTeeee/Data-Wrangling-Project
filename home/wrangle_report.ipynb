{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeRateDogs Twitter Data from 2015 to 2017\n",
    "\n",
    "___<a id=\"back\"></a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling Project - Report\n",
    "### By David Anifowoshe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#intro)\n",
    "2. [Gathering](#gathering)\n",
    "3. [Assessing](#assessing)\n",
    "4. [Cleaning](#cleaning)\n",
    "5. [Conclusions](#conclusions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction<a id=\"intro\"></a>\n",
    "For the Data Wrangling, we were tasked to examing twitter\n",
    "data for popular handle WeRateDogs [@dog_rates](https://twitter.com/dog_rates) and\n",
    "perform in-depth, but not exhaustive, data wrangling procedures on different sets of\n",
    "data. \n",
    "\n",
    "The end goal is to exhibit our data gathering, accessing, and cleaning abilities.\n",
    "The result would be few simple insights and a basic visualization, with more focus on\n",
    "the data wrangling process rather than the product. I'll go into brief detail on each step\n",
    "below.\n",
    "\n",
    "[Back to the top](#back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gathering<a id=\"gathering\"></a>\n",
    "Three data sources, which were turned into dataframes, were used for this project:\n",
    "1. \"twitter-archive-enhanced.csv\" → archive\n",
    "2. \"image-predictions.tsv\" → image\n",
    "3. \"tweet_json.txt\" → retweet\n",
    "\n",
    "(1) \"twitter-archive-enhanced.csv\" was made available for download in advance by\n",
    "Udacity. It contains about 3000 tweets and their date from 2015 to 2017. The\n",
    "data includes a tweet ID, tweet text, date tweeted, tweet URL, extracted dog ratings\n",
    "(typically out of 10, but with a numerator like 12 or 13 for good humor), the dog's name,\n",
    "so-called dog \"stage\" (such as the young \"puppo\" to the older \"doggo\"), and other data\n",
    "points. It was turned into a dataframe using pandas read_csv() function.\n",
    "This set later renamed to \"archive\"\n",
    "\n",
    "(2) \"image-predictions.tsv\" is a file prepared by Udacity, it needed to be\n",
    "downloaded via URL. Using the \"requests\" package and the \"get()\" function to access\n",
    "the file, I used the \"os package\" to open and write the file, then turned it into a dataframe\n",
    "using \"Pandas\". The file contains results from running the WeRateDogs tweet\n",
    "archive images through a neural network to try and classify the breeds of dogs. The\n",
    "resulting file contains a table of image preditions (top 3), and each corresponding tweet\n",
    "ID, image URL, and the image number that corresponded to the most confident prediction.\n",
    "This set was later renamed to \"image\"\n",
    "\n",
    "(3) \"tweet_json.txt\" is a text file in JSON format provided by Udacity. I had\n",
    "initially tried to open a developer account with Case# 0285639326 Twitter developer account application [ref:00DA0000000K0A8.5004w00002UdpaO:ref], but as diened, so I used the one from Udacity. Thank you for providing it.\n",
    "So I instead opted to use the provided data which I turned into a dataframe once again using Pandas. The file contains tweet IDs, retweet count, and favorite (\"like\") count.\n",
    "This file was renamed \"retweet\"\n",
    "\n",
    "[Back to the top](#back)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assessing<a id=\"assessing\"></a>\n",
    "We were instructed to \"detect and document at least eight quality issues and two tidiness\n",
    "issue\" using both visual and programmatic assessment. To assess the quality and\n",
    "tidyness of the data, I performed multiple data exploration functions on each dataframe.\n",
    "These functions included, but were not limited to:\n",
    "\n",
    "    sample()\n",
    "   Used this function to get a sampling of rows from each df to see how the columns and data were written, if there were any NaN entries, any unnecessary data or\n",
    "columns, any commonalities or differences between each dataset, and simply to\n",
    "see if any issues could be detected by browsing through entries.\n",
    "\n",
    "    info()\n",
    "Here I could see differences between column names and their respective\n",
    "datatypes. If something that should be a number but was an \"object\" rather than\n",
    "\"int64\", I could identify it here. I could also see differences in number of rows.\n",
    "\n",
    "    duplicated()\n",
    "Here I could string together several list() methods to detect if columns were\n",
    "duplicated between dataframes. Identifying such columns would be useful for\n",
    "merging and joining dfs if necessary.\n",
    "\n",
    "    describe()\n",
    "This function uses basic summary statistics to gather insights across the\n",
    "numerical data. Here I can see if say the max or min value of the rating numerator\n",
    "or denominator are suspiciously high or low.\n",
    "\n",
    "    value_counts()\n",
    "Here I can look at specific columns to count the values of all variables and have\n",
    "them shown in order. I can notice if certain values are suspiciously high or low.\n",
    "I identified the following issues:\n",
    "### Quality issues\n",
    "#### Dataframe Issue\n",
    "   __1. archive__\n",
    "    There are some rows that have \"retweet_status_values\", which means a duplicate tweet. The need to be removed.\n",
    "\n",
    "__2. archive__\n",
    "Unnecessary columns ( in_reply_to_status_id , in_reply_to_user_id ,\n",
    "retweeted_status_id , source , retweeted_status_id ,\n",
    "retweeted_status_user_id , retweeted_status_timestamp )\n",
    "\n",
    "__3 archive__\n",
    "Different tweet_id count from df_image (suggests some tweets in\n",
    "df_archive do not have images)\n",
    "\n",
    "__4 archive__ name column contains name ‘Noneʼ\n",
    "\n",
    "__5 archive__\n",
    "name column contains entries ‘aʼ and ‘quiteʼ (i.e. non-names that start with\n",
    "lower-case)\n",
    "\n",
    "__6 archive__ text column contains hyperlink info (starting with ‘httpsʼ)\n",
    "\n",
    "__7 archive__\n",
    "Remove url from text column to be more readable.\n",
    "\n",
    "__8 archive__  timestamp column is ‘objectʼ Dtype and ‘tweet_id' is 'int64' Dtype\n",
    "\n",
    "__9 retweet__\n",
    "  Drop all columns except for tweet_id, jpg_url, and p1 for the image dataset\n",
    "\n",
    "__10 image__  Has multiple image predictions when only one is necessary\n",
    "\n",
    "### Tidiness issues\n",
    "#### Dataframe Issue\n",
    "__1 archive__ Variables as column headers ( doggo , flooder , pepper , puppy )\n",
    "\n",
    "__2 tweets and image__\n",
    "Share same observational unit as df_archive so they don't need to be\n",
    "separate dataframes\n",
    "\n",
    "[Back to the Top](#back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning<a id=\"cleaning\"></a>\n",
    "Corresponding to the above quality and tidiness issues, I defined and fixed the issues as\n",
    "follows.\n",
    "Quality:\n",
    "1. Remove unnecessary retweets using boolean masking to select only entries that have\n",
    "null values (ie. that are \"True\") for retweeted_status_id\n",
    "2. Drop unnecessary columns\n",
    "3. Drop rows that are not common between df_archive and df_image using the\n",
    "isin() function to align the tweet_id count\n",
    "4. Examine name column entries that contain \"None\" to confirm that they are entered\n",
    "correctly, and then fix entries if necessary.\n",
    "5. Fix misentered names in the name column\n",
    "6. Remove hyperlink data from text column in the _dfarchive dataframe using regex and\n",
    "string splitting.\n",
    "7. For entries with irregular denominators (i.e. not 10), normalize both the numerator and\n",
    "denominator to a standard denominator of 10. For entries with irregular numerators\n",
    "(i.e. outliers outside of the 95th percentile but have denominators of 10), either\n",
    "8. normalize the entries using the overall median or fix an error\n",
    ". Change dtype of timestamp column to datatime using to_datetime\n",
    "9. Change dtype of tweet ID , retweet count , and favorite count to int\n",
    "using the astype function. Rename tweet ID to tweet_id so that it matches\n",
    "the naming convention of the other tables\n",
    "10. Drop all columns except for tweet_id , jpg_url , and p1 . Rename 'p1' to 'breed'.\n",
    "\n",
    "\n",
    "**Tidiness:**\n",
    "1. Extract dog stage names in text and, if found, add them to a new column\n",
    "dog_stages .\n",
    "2. Merge df_tweets_clean to df_archive_clean to create df_master . Merge\n",
    "df_image_clean to df_master\n",
    "\n",
    "[Back to the Top](#back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions<a id=\"conclusions\"></a>\n",
    "\n",
    "There was a lot of trial an error in making this code robust enough eg to find the dog names\n",
    "in text but not too far reaching so as to get noise. After examining the tweet text visually in\n",
    "a spreadsheet, I could see patterns like \"named dog name\", \"name is dog name\", and\n",
    "\"This is dog name\". So I intially included expanded code to extract all three of those\n",
    "combinations, but realized \"This is dog name\" found some names but more often found\n",
    "\"This is a dog breed so I got many \"a\" and \"an\" which was a problem I was fixing as a\n",
    "seperate issue. I revised my code and had better results.\n",
    "\n",
    "\n",
    "I also changed the order and added quality and tidiness issues as I went along. I did my\n",
    "intial assessment and listed my issues, but solving one issue often presented another. For\n",
    "example, I'd go back and move the 5th issue up to the 1st place because solving one issue\n",
    "would be best done earlier. The wrangling process was far from straightforward and I had\n",
    "to be flexible and adaptive.\n",
    "\n",
    "I came to the conclusion that I need to improve the speed and general python abilities.\n",
    "This project took longer than expected. That's down to my lack of experience but also the fact that data wrangling is a particularly time-consuming process. Once I had a clean, tidy\n",
    "master dataframe, I could easily pull out any insights from the data. The more time I put\n",
    "into wrangling, the lower chance I'll run in to problems at later stages.\n",
    "\n",
    "I also need to practise on the area of web scrapping, i didnt have the opportunity to do that in this project cause the Tweeter didnt give me the developer account, so i have already started and will studying more and practice web scrapping, infact I already got some videos from good data sciencetists on youtube, I know this area is very important for me to master.\n",
    "\n",
    " [Back to the Top](#back)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
